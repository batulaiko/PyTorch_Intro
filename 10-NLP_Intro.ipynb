{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7984bef8-210c-4a5f-a2c1-99df31ccef0e",
   "metadata": {},
   "source": [
    "# Natural Language Processing Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300b61e6-ccca-4aac-8dbc-b457b26d90a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lecturer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lecturer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# You need to run this once!\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf8be5-1760-42fd-93f2-3d1a19be583a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b3e4fd-183a-4187-84f0-1ceaf8af88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \\\n",
    "\"\"\"\n",
    "This is an example paragraph for tokenization. We will implement character and word-based tokenization from scratch using plain Python. \n",
    "Let's build vocabularies for both and use them to encode and decode the paragraph.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71895b0a-a91e-4fec-bc19-d3af38a528f6",
   "metadata": {},
   "source": [
    "## Manual Character based Tokenization\n",
    "* mapping each character to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c255a50f-86ea-4978-a2d6-e4c9728a1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_tokenization(text):\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c93b8-7ea8-4ff8-9dc3-710fe70a3ba6",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "* token to integer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9343fce3-7d82-4295-89eb-8e4439639ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokens):\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "            \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e487393-6298-4743-b688-aeff0220c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokens = character_tokenization(example_text)\n",
    "char_vocab = build_vocab(char_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151fd969-4877-4157-bdcd-71834e48873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens:\n",
      "['\\n', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'p', 'a', 'r', 'a', 'g', 'r', 'a', 'p', 'h', ' ', 'f', 'o', 'r', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', 'W', 'e', ' ', 'w', 'i', 'l', 'l', ' ', 'i', 'm', 'p', 'l', 'e', 'm', 'e', 'n', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 'w', 'o', 'r', 'd', '-', 'b', 'a', 's', 'e', 'd', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'f', 'r', 'o', 'm', ' ', 's', 'c', 'r', 'a', 't', 'c', 'h', ' ', 'u', 's', 'i', 'n', 'g', ' ', 'p', 'l', 'a', 'i', 'n', ' ', 'P', 'y', 't', 'h', 'o', 'n', '.', ' ', '\\n', 'L', 'e', 't', \"'\", 's', ' ', 'b', 'u', 'i', 'l', 'd', ' ', 'v', 'o', 'c', 'a', 'b', 'u', 'l', 'a', 'r', 'i', 'e', 's', ' ', 'f', 'o', 'r', ' ', 'b', 'o', 't', 'h', ' ', 'a', 'n', 'd', ' ', 'u', 's', 'e', ' ', 't', 'h', 'e', 'm', ' ', 't', 'o', ' ', 'e', 'n', 'c', 'o', 'd', 'e', ' ', 'a', 'n', 'd', ' ', 'd', 'e', 'c', 'o', 'd', 'e', ' ', 't', 'h', 'e', ' ', 'p', 'a', 'r', 'a', 'g', 'r', 'a', 'p', 'h', '.', '\\n']\n",
      "Character Vocabulary:\n",
      "{'\\n': 0, 'T': 1, 'h': 2, 'i': 3, 's': 4, ' ': 5, 'a': 6, 'n': 7, 'e': 8, 'x': 9, 'm': 10, 'p': 11, 'l': 12, 'r': 13, 'g': 14, 'f': 15, 'o': 16, 't': 17, 'k': 18, 'z': 19, '.': 20, 'W': 21, 'w': 22, 'c': 23, 'd': 24, '-': 25, 'b': 26, 'u': 27, 'P': 28, 'y': 29, 'L': 30, \"'\": 31, 'v': 32}\n"
     ]
    }
   ],
   "source": [
    "print('Character Tokens:')\n",
    "print(char_tokens)\n",
    "\n",
    "print('Character Vocabulary:')\n",
    "print(char_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ce0cb-2617-4bc5-92ca-6afeb3c1ae33",
   "metadata": {},
   "source": [
    "## Manual Word based Tokenization\n",
    "* mapping each word to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cec925-c88a-4da6-9e32-a028702ce9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(text):\n",
    "    #text = ''.join([char if char not in string.punctuation else ' ' for char in text])\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67399bcb-7822-4f98-8dd6-1427657f639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenization(example_text)\n",
    "word_vocab = build_vocab(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46ad543-b51a-4d79-8d8f-67ac8f1d6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens:\n",
      "['This', 'is', 'an', 'example', 'paragraph', 'for', 'tokenization.', 'We', 'will', 'implement', 'character', 'and', 'word-based', 'tokenization', 'from', 'scratch', 'using', 'plain', 'Python.', \"Let's\", 'build', 'vocabularies', 'for', 'both', 'and', 'use', 'them', 'to', 'encode', 'and', 'decode', 'the', 'paragraph.']\n",
      "Word Vocabulary:\n",
      "{'This': 0, 'is': 1, 'an': 2, 'example': 3, 'paragraph': 4, 'for': 5, 'tokenization.': 6, 'We': 7, 'will': 8, 'implement': 9, 'character': 10, 'and': 11, 'word-based': 12, 'tokenization': 13, 'from': 14, 'scratch': 15, 'using': 16, 'plain': 17, 'Python.': 18, \"Let's\": 19, 'build': 20, 'vocabularies': 21, 'both': 22, 'use': 23, 'them': 24, 'to': 25, 'encode': 26, 'decode': 27, 'the': 28, 'paragraph.': 29}\n"
     ]
    }
   ],
   "source": [
    "print('Word Tokens:')\n",
    "print(word_tokens)\n",
    "\n",
    "print('Word Vocabulary:')\n",
    "print(word_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd663e9d-d418-495a-be09-0bd1066e5480",
   "metadata": {},
   "source": [
    "### Encoding and Decoding of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6b4c69-420c-42ea-805e-63023c4e057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_idx(text, vocab):\n",
    "    return [vocab[token] for token in text]\n",
    "\n",
    "def idx_to_tokens(indices, vocab):\n",
    "    return ' '.join([key for key, value in vocab.items() if value in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a22e6c35-fc36-446d-a6a5-4662d2716e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "This is an example paragraph for tokenization. We will implement character and word-based tokenization from scratch using plain Python. \n",
      "Let's build vocabularies for both and use them to encode and decode the paragraph.\n",
      "\n",
      "Encoded Word Indices:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 5, 22, 11, 23, 24, 25, 26, 11, 27, 28, 29]\n",
      "Decoded Word Text:\n",
      "This is an example paragraph for tokenization. We will implement character and word-based tokenization from scratch using plain Python. Let's build vocabularies both use them to encode decode the paragraph.\n"
     ]
    }
   ],
   "source": [
    "encoded_word = token_to_idx(word_tokens, word_vocab)\n",
    "decoded_word = idx_to_tokens(encoded_word, word_vocab)\n",
    "\n",
    "print('Original Text:')\n",
    "print(example_text)\n",
    "print('Encoded Word Indices:')\n",
    "print(encoded_word)\n",
    "print('Decoded Word Text:')\n",
    "print(decoded_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098d59b-7b7e-4b98-b6d9-f7dd528fed1f",
   "metadata": {},
   "source": [
    "## Word Tokenization With NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b02baee-3152-4c75-8d78-ee9f2ae6a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Few', 'foxes', 'are', 'running', 'towards', 'us', '!']\n"
     ]
    }
   ],
   "source": [
    "text = 'Few foxes are running towards us!'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print('Tokens:', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f31f09-5ef1-40ee-92c7-d6aacf29dcda",
   "metadata": {},
   "source": [
    "# Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f73f70df-0c76-4c6c-a8b7-5b922224a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['foxes', 'running', 'towards', 'us', '!']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe7e96-5df7-4956-8319-a7601644e8f5",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "* Stemming is a text normalization technique that involves reducing words to their base or root form, known as the \"stem.\" \n",
    "* The process involves removing suffixes or prefixes from words, aiming to obtain the root form that captures the core meaning\n",
    "* \"running\" -> \"runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6e66fb-f087-4085-bbe6-a2ec2845167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['fox', 'run', 'toward', 'us', '!']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print('Stemmed Tokens:', stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c038d3-069f-40fa-aacc-4d088c63aa25",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "* Lemmatization is also a text normalization technique, but it focuses on reducing words to their base or dictionary form, known as the \"lemma.\" \n",
    "* Unlike stemming, lemmatization considers the context of words and their part of speech to provide a meaningful transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25bcc1be-27b1-41a6-961c-ad4003c73a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['fox', 'running', 'towards', 'u', '!']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "print('Lemmatized Tokens:', lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b20ce-7c74-4632-be9b-d23e06212dd7",
   "metadata": {},
   "source": [
    "# Sparse Vectoral Representation\n",
    "* **Numerical Input for Models:** Machine learning models, including neural networks, require numerical input.\n",
    "* Vector representations convert textual data into numerical format, enabling models to process and learn from the data.\n",
    "* **Sparse vectors are representations where the majority of elements are zero, and only a small number of non-zero elements carry meaningful information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1d63f-fb62-47de-81f2-11842d442583",
   "metadata": {},
   "source": [
    "### Bag-of-Words (BoW) model\n",
    "*  It represents a document as an unordered set of words, disregarding grammar and word order but keeping track of the frequency of each word.\n",
    "* **corpus:** collection of text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de8bed69-00fa-4185-9858-8fd6a6b62bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Matrix:\n",
      "[[0 1 0 0 0 1 1 1]\n",
      " [1 1 1 1 0 0 0 0]\n",
      " [0 0 0 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is a simple example.', \n",
    "    'Another example for illustration.', \n",
    "    'Illustration is important.'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"BoW Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5a23e-a29a-494a-8d5a-a8ca407df654",
   "metadata": {},
   "source": [
    "### TF-IDF representation\n",
    "* TF-IDF is a text representation technique that takes into account both the frequency of a term in a document (Term Frequency) and its importance in the entire corpus (Inverse Document Frequency). \n",
    "* It aims to highlight terms that are distinctive to a document while downweighting common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88027d9b-10ca-4c78-b870-23dfe1046592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.42804604 0.         0.         0.         0.42804604\n",
      "  0.5628291  0.5628291 ]\n",
      " [0.5628291  0.42804604 0.5628291  0.42804604 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.51785612 0.68091856 0.51785612\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print('TF-IDF Matrix:')\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091de96-e9fa-4c26-a147-7168cc7c8a00",
   "metadata": {},
   "source": [
    "## Dense Vectoral Representation\n",
    "*  **Dense vectors are representations where most elements contain non-zero values, and each element typically contributes to the overall representation.**\n",
    "### Word2Vec representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "811a90e5-935e-4cdf-8c88-af7e1acae66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding for 'language':\n",
      "[-0.08157917  0.04495798 -0.04137076  0.00824536  0.08498619 -0.04462177\n",
      "  0.045175   -0.0678696  -0.03548489  0.09398508]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# word tokenized corpus\n",
    "corpus = [\n",
    "    ['natural', 'language', 'processing'], \n",
    "    ['word', 'embeddings', 'are', 'interesting']\n",
    "]\n",
    "\n",
    "model = Word2Vec(corpus, vector_size=10, window=2, min_count=1, workers=4)\n",
    "\n",
    "# get the word vectors (embbedings)\n",
    "word_embeddings = model.wv\n",
    "\n",
    "print(\"Word Embedding for 'language':\") \n",
    "print(word_embeddings['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826c692-4916-4415-8076-844372ebe0d2",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "429c2e2e-8ab9-4e21-b4d5-6e3425e442cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 12:45:04.371749: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-09 12:45:04.373177: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-09 12:45:04.394381: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-09 12:45:04.394405: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-09 12:45:04.394420: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-09 12:45:04.401696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-09 12:45:04.903934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-09 12:45:05.457915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-09 12:45:05.458629: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# You need to run this once!\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875754f-028b-496d-8e46-8a37210ba2fc",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb2f51d4-1581-4dbf-96b9-f4b8e085c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [('Windows 11', 'PRODUCT')]\n"
     ]
    }
   ],
   "source": [
    "#sentence = \"Let's meet with Jonh at Istanbul on this Monday at 9:00 PM\"\n",
    "sentence = \"Linux is way better and Windows 11\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "print('Named Entities:', entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16a9493f-39ef-4641-9d0b-719ab3c38774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Linux is way better and \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Windows 11\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True, options={'compact': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3992e96-b797-4b1f-9db5-bb54f40fc5a0",
   "metadata": {},
   "source": [
    "### Dependency Tree Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9dc7755-28ba-4c5c-b4a4-c05b079b519b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4bafddd99d37494188d35bed587a40ad-0\" class=\"displacy\" width=\"890\" height=\"377.0\" direction=\"ltr\" style=\"max-width: none; height: 377.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Linux</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">way</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">better</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">Windows</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">11</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bafddd99d37494188d35bed587a40ad-0-0\" stroke-width=\"2px\" d=\"M70,242.0 C70,182.0 155.0,182.0 155.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bafddd99d37494188d35bed587a40ad-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,244.0 L62,232.0 78,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bafddd99d37494188d35bed587a40ad-0-1\" stroke-width=\"2px\" d=\"M310,242.0 C310,182.0 395.0,182.0 395.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bafddd99d37494188d35bed587a40ad-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,244.0 L302,232.0 318,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bafddd99d37494188d35bed587a40ad-0-2\" stroke-width=\"2px\" d=\"M190,242.0 C190,122.0 400.0,122.0 400.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bafddd99d37494188d35bed587a40ad-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,244.0 L408.0,232.0 392.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bafddd99d37494188d35bed587a40ad-0-3\" stroke-width=\"2px\" d=\"M190,242.0 C190,62.0 525.0,62.0 525.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bafddd99d37494188d35bed587a40ad-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M525.0,244.0 L533.0,232.0 517.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bafddd99d37494188d35bed587a40ad-0-4\" stroke-width=\"2px\" d=\"M190,242.0 C190,2.0 650.0,2.0 650.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bafddd99d37494188d35bed587a40ad-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M650.0,244.0 L658.0,232.0 642.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bafddd99d37494188d35bed587a40ad-0-5\" stroke-width=\"2px\" d=\"M670,242.0 C670,182.0 755.0,182.0 755.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bafddd99d37494188d35bed587a40ad-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M755.0,244.0 L763.0,232.0 747.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f78754-83f7-4d6d-ac05-76b29c836744",
   "metadata": {},
   "source": [
    "### Keyword Extraction using Part of Speech (POS) and NNP tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "620d5efc-55e3-4736-b581-e0a6e629a0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords (NNP): ['Linux', 'Windows']\n"
     ]
    }
   ],
   "source": [
    "#example_text = 'Apple Inc. is planning to open a new store.'\n",
    "example_text = \"Linux is way better and Windows 11\"\n",
    "\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Extract keywords (NNP)\n",
    "keywords = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "print('Keywords (NNP):', keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71480e2b-2de4-4188-a28e-410df0b1037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_paragraph = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence\n",
    "that focuses on the interaction between computers and humans using natural\n",
    "language. It encompasses the development of algorithms and models to enable computers to\n",
    "understand, interpret, and generate human-like text. NLP plays a crucial role in various\n",
    "applications, including chatbots, sentiment analysis, language translation, and information\n",
    "retrieval.  In recent years, there has been a tremendous growth in the adoption of NLP\n",
    "techniques due to advancements in machine learning and deep learning. These techniques\n",
    "allow NLP models to capture more complex linguistic patterns and nuances, making\n",
    "them highly effective in tasks such as named entity recognition, text\n",
    "summarization, and question answering. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df9ecf7f-cfd4-46da-ba79-bba143488e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords (NNP): ['Natural', 'Language', 'Processing', 'NLP', 'NLP', 'NLP', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(example_paragraph)\n",
    "keywords = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "print('Keywords (NNP):', keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a762feb-54ef-4b5b-8b35-010771cf57f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Keywords:\n",
      "NLP: 4 occurrences\n",
      "Natural: 1 occurrences\n"
     ]
    }
   ],
   "source": [
    "keyword_counts = Counter(keywords)\n",
    "\n",
    "most_common_keywords = keyword_counts.most_common(n=2)\n",
    "\n",
    "print('Most Common Keywords:')\n",
    "\n",
    "for keyword, count in most_common_keywords:\n",
    "    print(f\"{keyword}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4ac2d95-ac91-4d68-8392-61fb4cad02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_paragraph_2 = \"\"\"\n",
    "The latest flagship smartphone, the Galaxy Pro, stands out as a true\n",
    "marvel in the ever-evolving world of mobile technology. Boasting a sleek and premium\n",
    "design, the phone's glass back seamlessly curves into a metal frame, providing a\n",
    "comfortable grip and a sophisticated aesthetic. The vibrant 6.5-inch Super AMOLED display\n",
    "offers stunning clarity and vibrant colors, making every video and image a visual\n",
    "delight. The Galaxy Pro is powered by the latest octa-core processor and 8GB of\n",
    "RAM, ensuring seamless multitasking and smooth performance. The camera setup is\n",
    "nothing short of impressive, with a triple-lens system that captures sharp and\n",
    "detailed photos in various lighting conditions. The 4500mAh battery provides all-day\n",
    "longevity, and the inclusion of fast charging ensures you spend more time enjoying your\n",
    "device and less time waiting for it to power up. The user interface is intuitive,\n",
    "running on the latest version of the Galaxy OS, with a host of customizable features that\n",
    "cater to both tech enthusiasts and casual users alike. Overall, the Galaxy Pro\n",
    "sets a new standard for flagship smartphones, combining cutting-edge technology\n",
    "with a refined design for a truly exceptional user experience\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9b30fb5-1665-48c2-8951-bb6f56bf3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords (NNP): ['Galaxy', 'Pro', 'Galaxy', 'Pro', 'GB', 'Galaxy', 'OS', 'Galaxy']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(example_paragraph_2)\n",
    "keywords_2 = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "print('Keywords (NNP):', keywords_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a218076b-6059-4d81-8716-e623eda61404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Keywords:\n",
      "Galaxy: 4 occurrences\n",
      "Pro: 2 occurrences\n"
     ]
    }
   ],
   "source": [
    "keyword_counts_2 = Counter(keywords_2)\n",
    "\n",
    "most_common_keywords_2 = keyword_counts_2.most_common(n=2)\n",
    "\n",
    "print('Most Common Keywords:')\n",
    "\n",
    "for keyword, count in most_common_keywords_2:\n",
    "    print(f\"{keyword}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb353a-73d6-41d6-819e-e825db8ea3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
