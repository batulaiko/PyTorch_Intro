{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e538dfc0-a01f-454f-9f50-e21a6bb1e5d3",
   "metadata": {},
   "source": [
    "# PyTorch Models Intro\n",
    "### Building Deep Learning Models using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcbc5a0-be86-46d7-84bb-65d013b6a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0a713-54d9-49dd-a1bf-e3790d9cdd3f",
   "metadata": {},
   "source": [
    "# Basic Linear Classifier\n",
    "* $y = w_1x_1 + w_2x_2 + w_3x_3 + \\dots + w_0$\n",
    "* $w_i$ and $w_0$ are the trainable parameters\n",
    "* $x_i$ is the input (data), $y$ is the output (prediction)\n",
    "* All models (and layers) are subclass of `nn.Module` in PyTorch\n",
    "* `nn.Parameter()` is the basic learnable parameter inside the module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cae22e-6c8c-49a1-89fe-e7316f4c1f41",
   "metadata": {},
   "source": [
    "## Using Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d165677-6986-4954-ac2b-88b7f99bd51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLinearDot(nn.Module):\n",
    "    def __init__(self, features_dim):\n",
    "        # YOU NEED TO RUN FOR INITIALIZATION OF MODULE\n",
    "        super().__init__()\n",
    "\n",
    "        # bias\n",
    "        self.w_0 = nn.Parameter(torch.zeros(1))\n",
    "        # weights\n",
    "        self.w_1 = nn.Parameter(torch.randn(features_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.dot(self.w_1, x) + self.w_0\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c6059c-9d16-47a2-9674-e5888a35c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dot = BasicLinearDot(features_dim=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0963db1f-d3f0-42bd-949b-bbf702cd1236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_0: Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "\n",
      "w_1: Parameter containing:\n",
      "tensor([-4.6346e-01, -7.3105e-01, -4.6454e-01,  9.5200e-01, -1.4984e+00,\n",
      "         2.6727e-02, -1.6820e+00, -3.8680e-01,  1.2621e+00,  8.9513e-02,\n",
      "         4.5511e-01,  7.5176e-01,  1.9143e+00,  1.0037e+00,  1.6121e-01,\n",
      "        -1.6499e+00, -1.8510e+00, -1.6325e+00,  1.0073e+00,  2.5170e-04],\n",
      "       requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model_dot.named_parameters():\n",
    "    print(f'{name}: {parameter}')\n",
    "    print() # empty line to improve the readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbe0a1-cba3-420c-9fdc-fb7fe7908093",
   "metadata": {},
   "source": [
    "### Test with dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a99fc74-60e9-4cc9-adb0-936f1184b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = 20\n",
    "\n",
    "dummy_data = torch.randn(features)\n",
    "dummy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b3ec26-4a05-4e30-b838-dcf0ab93593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_dot(dummy_data)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5db62d-d076-4e7d-b294-05f0e93b28d9",
   "metadata": {},
   "source": [
    "### Test with batched dummy data\n",
    "* Shape: (N, F)\n",
    "* *N* is the batch size, *F* is the features dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff0aefc-2325-494f-8046-bf987fdfa831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "features = 20\n",
    "\n",
    "dummy_data_batch = torch.randn(batch_size, features)\n",
    "dummy_data_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440f454-aa04-4210-8764-d21559d71d98",
   "metadata": {},
   "source": [
    "### Dot product doesn't work anymore!\n",
    "* https://pytorch.org/docs/stable/generated/torch.dot.html\n",
    "* Dot product only work 1D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74a0ea4-8676-449f-9973-af2ee068dc28",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 1D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m model_dot(dummy_data_batch)\n\u001b[0;32m      2\u001b[0m pred\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\batuy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\batuy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36mBasicLinearDot.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_1, x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_0\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D tensors expected, but got 1D and 2D tensors"
     ]
    }
   ],
   "source": [
    "pred = model_dot(dummy_data_batch)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fee384-b285-4c44-9c1a-092e3f1d7467",
   "metadata": {},
   "source": [
    "## Using Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d42928-7a24-47b7-81b3-2bb7eebb24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLinearMM(nn.Module):\n",
    "    def __init__(self, features_dim):\n",
    "        # YOU NEED TO RUN FOR INITIALIZATION OF MODULE\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_0 = nn.Parameter(torch.zeros(1))\n",
    "        # 2D parameter are required for matrix multiplication!\n",
    "        self.w_1 = nn.Parameter(torch.randn(1, features_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (N, F)\n",
    "        # w_1.shape: (1, F)\n",
    "        # x * w_1.T -> (N, F) * (F, 1)\n",
    "        # .T mean transpose\n",
    "        y = torch.mm(x, self.w_1.T) + self.w_0\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428c9042-a283-4ba6-bdcd-304a597b59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mm = BasicLinearMM(features_dim=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee22972-eb49-475c-a5ca-aa861045d6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy_data_batch.shape: (N, F)\n",
    "pred = model_mm(dummy_data_batch)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710169c5-e462-449d-92cb-ac63b7122d89",
   "metadata": {},
   "source": [
    "## What if we need a specific output feature dim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2f0cb-c268-4841-8c10-46b64f0b2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLinearMMImproved(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # YOU NEED TO RUN FOR INITIALIZATION OF MODULE\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_0 = nn.Parameter(torch.zeros(out_features))\n",
    "        # 2D parameter are required for matrix multiplication!\n",
    "        self.w_1 = nn.Parameter(torch.randn(out_features, in_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (N, F_in)\n",
    "        # w_1.shape: (F_out, F_in)\n",
    "        # x * w_1.T -> (N, F_in) * (F_in, F_out)\n",
    "        # .T mean transpose\n",
    "        y = torch.mm(x, self.w_1.T) + self.w_0\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e9e621e-9cd9-4176-a834-f64e99233954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mm_improved = BasicLinearMMImproved(in_features=20, out_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5017eba2-e661-4ef3-8a60-6c5cc2a540c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy_data_batch.shape: (N, F)\n",
    "pred = model_mm_improved(dummy_data_batch)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963aedf8-c72e-4490-8c74-5c20cce9897b",
   "metadata": {},
   "source": [
    "# What if we have high dimentional data (3 or more)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f48917-64ec-4408-bbd0-964fc38fc6f9",
   "metadata": {},
   "source": [
    "### Test with higher dimentional batched dummy data\n",
    "* Shape: (N, S, F)\n",
    "* *N* is the batch size, *S* sequence length, *F* is the features dim\n",
    "\n",
    "#### Information:\n",
    "Some data's shapes are presented as below with batches:\n",
    "\n",
    "- (N, S, F) for `textual`\n",
    "- (N, C, H, W) for `image`\n",
    "- (N, S, C, H, W) for `video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dead45bb-139f-4725-b4e1-3a96364f1059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 20])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "sequence_length = 10\n",
    "features = 20\n",
    "\n",
    "dummy_multidim_data_batch = torch.randn(batch_size, sequence_length, features)\n",
    "dummy_multidim_data_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af7fce-6fae-4740-a5cb-efbd46dfe497",
   "metadata": {},
   "source": [
    "### Matrix multiplication doesn't work anymore!\n",
    "* https://pytorch.org/docs/stable/generated/torch.mm.html\n",
    "* Matrix multiplication only work 2D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04b8419a-56c4-4f6d-9cee-ac84c0fe9459",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "self must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# dummy_data_batch.shape: (N, S, F)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pred \u001b[38;5;241m=\u001b[39m model_mm_improved(dummy_multidim_data_batch)\n\u001b[0;32m      3\u001b[0m pred\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\batuy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\batuy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mBasicLinearMMImproved.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# x.shape: (N, F_in)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# w_1.shape: (F_out, F_in)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# x * w_1.T -> (N, F_in) * (F_in, F_out)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# .T mean transpose\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_1\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_0\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[1;31mRuntimeError\u001b[0m: self must be a matrix"
     ]
    }
   ],
   "source": [
    "# dummy_data_batch.shape: (N, S, F)\n",
    "pred = model_mm_improved(dummy_multidim_data_batch)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e89182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0213e+00, -4.5882e-02,  2.2614e+00, -4.0166e-01, -7.1017e-01,\n",
       "          1.3010e+00, -1.0071e+00, -1.5348e+00, -4.1546e-02, -2.2139e-01,\n",
       "         -2.3881e-01,  2.1980e-01,  7.7252e-02,  4.3983e-02, -4.8397e-02,\n",
       "          1.8628e+00,  7.2579e-02,  6.4934e-01,  1.7202e+00, -9.9910e-01],\n",
       "        [-1.6038e+00, -5.3442e-01, -1.3830e+00, -1.1762e+00,  5.0216e-01,\n",
       "         -1.6949e+00,  1.0289e+00, -2.0343e+00, -3.0293e-01, -4.3212e-01,\n",
       "         -5.8980e-01,  2.5014e-01,  8.2625e-01,  2.8508e-01,  5.2712e-01,\n",
       "         -9.5925e-01,  1.0689e+00,  1.4853e+00, -2.5934e+00, -1.8201e+00],\n",
       "        [-3.4345e-01, -9.9762e-01, -7.8091e-02, -7.7035e-01, -3.7791e-01,\n",
       "         -3.3490e+00,  7.1416e-01, -3.5499e-01, -1.5817e+00,  5.5390e-01,\n",
       "         -2.2393e+00,  1.3136e+00, -1.0834e+00, -3.1529e+00,  3.1122e+00,\n",
       "          9.0029e-01,  7.0263e-01, -1.5140e-01,  4.7981e-01, -2.1705e-01],\n",
       "        [ 5.7410e-01,  1.0395e+00, -4.4477e-01,  1.5567e+00,  1.3732e+00,\n",
       "          9.5662e-01, -8.0153e-01, -1.4140e-01,  9.8192e-01, -1.6023e+00,\n",
       "         -2.5182e-01, -6.0034e-01, -3.1095e-01, -1.2842e+00, -2.3759e-01,\n",
       "         -7.6746e-02, -1.0976e+00,  1.0758e+00,  1.3776e+00, -1.1080e-01],\n",
       "        [-8.3060e-01, -3.9061e-01,  2.0399e+00, -1.4205e+00, -1.0859e-01,\n",
       "         -5.5117e-01,  6.3403e-01, -1.4064e+00,  6.3908e-02,  4.9312e-01,\n",
       "          1.2923e+00, -1.9029e-01, -8.9962e-03, -5.2420e-01,  5.2823e-01,\n",
       "         -2.6037e+00, -4.7373e-01,  2.3335e+00,  4.0676e-01, -9.4127e-01],\n",
       "        [ 2.1202e-02, -1.3334e+00,  1.1093e+00, -1.6429e-01,  1.7473e+00,\n",
       "         -1.2962e+00, -6.7510e-01, -8.4015e-01,  9.0677e-01,  1.4738e-02,\n",
       "         -1.2842e+00,  8.8091e-01,  8.9293e-01,  2.3444e+00,  5.4140e-01,\n",
       "          9.3086e-01,  8.7760e-01, -1.1746e+00, -6.8687e-01, -4.9404e-01],\n",
       "        [ 5.6013e-01, -1.0295e+00,  1.9163e+00, -6.7646e-01,  1.6059e+00,\n",
       "         -9.9302e-01,  2.2268e+00, -7.3337e-01, -3.6624e-03,  7.3438e-01,\n",
       "          2.4460e-02, -2.5305e-01, -3.5967e-01,  7.6539e-01,  6.7000e-01,\n",
       "          1.0131e+00,  4.2301e-01,  5.4231e-01, -2.0983e+00, -1.4004e+00],\n",
       "        [-2.3487e-01,  6.7753e-02, -1.0636e+00,  6.7271e-01,  9.8895e-02,\n",
       "         -6.1043e-01, -7.9761e-01,  2.7508e+00,  1.8973e+00,  2.3242e-01,\n",
       "          5.2646e-01,  1.0932e+00, -8.3420e-01, -6.1066e-01, -2.9811e-01,\n",
       "         -5.1960e-01, -7.9959e-01, -1.1377e+00, -2.4380e+00, -5.0042e-02],\n",
       "        [-3.7978e-02, -1.3052e+00,  1.2239e+00, -1.1759e-01, -1.5761e+00,\n",
       "         -3.7035e-01,  3.9636e-01,  2.7756e-01,  1.5548e+00, -1.1714e+00,\n",
       "         -4.7208e-02,  1.6402e+00, -3.2190e-01,  8.6732e-01,  3.6557e-01,\n",
       "         -1.4149e-01,  1.1709e+00, -2.3266e+00,  2.8494e-01, -1.7484e+00],\n",
       "        [ 2.5962e-01, -6.1602e-01, -1.4155e+00, -8.3677e-01,  4.4991e-01,\n",
       "         -4.4140e-01,  5.4995e-01, -1.0127e+00,  1.3949e+00, -3.8241e-01,\n",
       "          3.8201e-01, -1.1269e+00, -1.9244e+00, -9.5847e-01, -1.0041e-01,\n",
       "          1.3086e+00,  5.3825e-01,  5.2950e-01,  5.1487e-01,  3.6068e-01],\n",
       "        [ 3.3904e-01, -1.8830e+00,  5.8475e-01, -5.4170e-01, -1.0565e+00,\n",
       "         -1.5394e-01, -1.1080e-01, -5.5931e-01,  9.0557e-01, -2.8345e-01,\n",
       "          8.6587e-01, -9.7900e-01,  6.4519e-01, -1.4654e-01, -8.9876e-01,\n",
       "         -9.7390e-01,  1.7085e+00,  1.1320e+00,  1.6534e+00, -8.3132e-01],\n",
       "        [-3.4437e-02, -5.9994e-01,  1.4785e+00, -6.3479e-01, -6.1584e-01,\n",
       "          4.3990e-01, -6.0907e-01, -1.0664e+00,  1.2443e+00,  1.0234e+00,\n",
       "         -4.0257e-01,  5.8527e-01,  3.5157e-02, -6.8388e-01,  1.5570e+00,\n",
       "          2.5825e-01,  4.9031e-01,  1.6322e+00,  5.4395e-01, -1.0985e+00],\n",
       "        [-2.0598e-01,  1.0868e+00,  1.0942e+00, -9.2401e-01, -8.5100e-02,\n",
       "         -3.7905e-01,  5.8539e-01, -1.4241e-01,  8.4775e-01,  1.4904e-01,\n",
       "         -2.2927e-01,  1.8919e+00, -4.5904e-02,  7.2545e-01, -9.2748e-01,\n",
       "          1.2788e+00, -3.7166e-01,  8.6538e-01,  1.0125e-01,  8.1862e-01],\n",
       "        [ 9.6297e-01, -1.5627e+00, -9.8577e-01, -1.2210e+00, -9.4888e-01,\n",
       "          6.6278e-01,  3.3908e-01, -2.3133e+00, -1.0202e+00, -3.7445e-01,\n",
       "         -4.5791e-01,  6.1933e-01, -7.7090e-01,  1.0996e+00,  1.4478e+00,\n",
       "          9.3467e-01, -5.6928e-01,  2.6808e-01,  7.2433e-01, -1.5158e+00],\n",
       "        [-2.0280e-01,  2.2415e+00,  5.5172e-01, -6.3311e-01, -2.4141e-01,\n",
       "         -9.6390e-01,  2.0960e+00,  7.7666e-01,  5.5576e-01,  2.0602e-01,\n",
       "          9.5714e-01, -2.4119e-01,  1.0300e+00, -9.8020e-01,  3.7895e-01,\n",
       "         -8.6687e-01,  1.0273e+00, -1.9810e-01, -4.3951e-01,  1.0694e+00],\n",
       "        [-1.4937e+00,  9.7281e-01,  1.9747e-01, -1.5251e+00, -1.3709e+00,\n",
       "          1.5919e-02, -8.4130e-01,  9.3009e-01,  6.4578e-01, -9.9838e-01,\n",
       "          2.1222e-01, -4.6926e-01, -1.1594e+00, -2.8026e+00,  6.4306e-01,\n",
       "          5.1035e-01, -1.0423e+00,  6.1778e-01, -8.5474e-01,  1.4513e+00],\n",
       "        [ 3.6734e-02,  2.2408e+00,  2.8313e-01, -8.4899e-01, -1.0641e+00,\n",
       "         -1.5767e-01,  9.2753e-01,  1.0195e-01, -5.6283e-02, -4.7941e-01,\n",
       "          1.9675e-01, -1.4453e+00,  8.6835e-01,  2.8880e-01,  1.3707e+00,\n",
       "          1.9420e-01, -1.0391e-01,  4.8826e-01,  1.4371e+00,  5.7905e-01],\n",
       "        [ 1.1019e+00,  8.7095e-01, -2.4677e-02,  8.1379e-01,  1.9957e+00,\n",
       "          1.2021e+00,  9.5003e-01, -2.6347e-01, -2.5773e-01,  1.1548e+00,\n",
       "          1.2498e-01,  5.8443e-02,  5.8125e-01,  7.2824e-01,  5.9990e-03,\n",
       "         -2.1839e+00, -2.0204e-01,  1.2094e-01,  5.9031e-01,  2.0791e+00],\n",
       "        [ 9.1789e-01, -7.7988e-01,  3.4352e-02, -5.8513e-01, -1.5673e+00,\n",
       "          4.8661e-01, -2.1826e+00,  5.0783e-01,  9.5126e-01,  5.5863e-01,\n",
       "         -3.3335e-01, -1.0243e+00,  4.8488e-01,  1.1563e+00,  6.9983e-01,\n",
       "          4.5680e-01, -5.4259e-01,  4.8677e-01,  7.9696e-01,  1.1295e+00],\n",
       "        [ 2.3925e+00, -1.7896e+00, -2.4022e+00, -4.3360e-01, -7.7450e-01,\n",
       "         -4.4128e-01, -1.3352e+00,  9.9665e-01, -1.5021e+00,  6.1664e-01,\n",
       "          3.5780e-01,  1.8735e+00,  5.7835e-01,  1.0276e+00,  6.9527e-01,\n",
       "         -8.1484e-01,  7.5049e-01, -6.6648e-02,  5.6986e-01,  2.9711e-01],\n",
       "        [-8.4658e-01,  5.5732e-01, -7.9596e-01,  1.4219e-01, -8.6344e-01,\n",
       "         -6.3509e-01,  3.0331e-01,  2.6699e-01,  2.9172e-01,  7.7030e-01,\n",
       "          6.1024e-01, -1.0347e+00,  2.7156e-01, -1.1808e-01, -3.2946e-01,\n",
       "         -8.0893e-01, -8.3039e-01,  3.6197e-01,  9.8746e-01, -9.4620e-02],\n",
       "        [-5.2157e-01,  3.5842e-01,  3.9143e-01,  7.2764e-01, -1.7959e-01,\n",
       "         -1.2742e+00,  1.4186e-01,  4.2041e-02, -7.6399e-02,  2.2654e-01,\n",
       "          3.6455e-01,  6.1907e-02,  1.3074e-01,  3.7244e-01, -1.7539e+00,\n",
       "         -2.8092e-01,  7.9920e-01,  3.1589e-01, -1.7486e-01,  1.6514e+00],\n",
       "        [ 7.4188e-01, -4.2539e-01, -7.0461e-01, -4.0468e-01,  1.6825e+00,\n",
       "          1.6990e+00,  1.1750e+00, -3.8752e-01, -4.8268e-01,  9.1764e-01,\n",
       "          7.9044e-01,  1.4597e+00, -4.7761e-01,  4.4750e-01, -6.6785e-01,\n",
       "          2.4271e+00,  1.0241e+00,  3.0249e-01, -4.5588e-01,  1.2611e+00],\n",
       "        [ 2.4818e-01, -7.1518e-01, -2.6053e-01, -5.2946e-01, -1.3311e+00,\n",
       "          4.5966e-01, -2.0123e+00, -1.0732e+00,  2.7189e-01,  5.5321e-01,\n",
       "         -4.4255e-01, -1.0250e+00,  4.5628e-01,  1.2314e+00, -2.1532e+00,\n",
       "          6.7460e-01,  9.2629e-01, -4.1588e-01,  3.4659e-01, -5.8938e-01],\n",
       "        [ 5.7544e-01,  1.4464e-01, -7.6698e-01,  1.4557e+00, -3.4005e-01,\n",
       "          6.5877e-01, -2.5581e+00,  5.9054e-01, -1.3623e+00, -1.1389e-01,\n",
       "         -8.0523e-01,  1.8396e+00, -3.3351e-01, -1.3833e+00,  2.8298e-02,\n",
       "         -1.1770e+00,  3.8038e-01,  1.7427e+00, -1.4647e+00, -1.4195e-01],\n",
       "        [-1.5605e+00,  7.6919e-01,  8.5227e-01, -1.1228e-01,  4.2827e-01,\n",
       "          4.0361e-01, -1.0421e+00, -1.4779e+00,  1.4621e+00, -6.6490e-01,\n",
       "         -1.6409e-01, -2.1673e+00, -8.8155e-01, -3.1641e-01,  1.1949e+00,\n",
       "         -2.5471e-01,  2.5213e-01,  8.1775e-02,  2.0030e+00, -2.0018e+00],\n",
       "        [-5.9223e-02, -2.1753e-02, -3.0640e-01, -1.2029e+00,  9.5085e-01,\n",
       "         -8.2596e-01,  5.5232e-01, -3.2953e-02, -1.6673e+00, -1.5188e+00,\n",
       "          1.4113e+00, -7.4680e-01,  9.7181e-02, -6.5967e-01,  8.8440e-01,\n",
       "         -1.0833e+00,  4.5287e-01, -7.5614e-01, -1.3049e+00, -3.0299e+00],\n",
       "        [-4.8132e-01,  4.5948e-01, -2.4172e-01, -9.0656e-01,  1.5360e+00,\n",
       "         -1.0259e+00,  8.1486e-01,  8.5752e-01, -1.5845e+00,  1.9426e-01,\n",
       "         -6.8917e-01, -1.1282e-01, -7.2807e-01, -2.2914e-01, -1.4543e+00,\n",
       "          3.3930e-01,  4.9755e-01,  2.5979e-03,  1.9091e+00, -1.0524e+00],\n",
       "        [ 1.1375e+00, -2.2231e+00,  1.6593e-01,  7.0518e-04, -6.6699e-01,\n",
       "         -1.8829e+00,  2.3411e+00, -5.3630e-01, -2.7989e+00,  6.9377e-01,\n",
       "          1.2422e+00,  5.6648e-01, -8.3201e-01, -6.3764e-01,  6.9485e-01,\n",
       "         -1.2535e+00,  2.3385e+00,  1.7910e-01,  1.5315e+00, -7.1858e-01],\n",
       "        [ 1.3573e+00,  1.7027e+00, -6.3954e-01,  1.2322e+00,  1.2366e+00,\n",
       "          2.5296e-01,  6.9432e-01,  1.4157e+00, -9.5629e-01, -1.0442e+00,\n",
       "         -8.6094e-01, -8.1557e-02,  1.7621e-01, -1.5219e+00, -3.9640e-01,\n",
       "         -9.2150e-01, -1.2259e+00,  3.6800e-01, -6.6513e-01, -6.1871e-01],\n",
       "        [ 8.5452e-01,  9.8512e-01, -1.4006e+00, -8.8928e-01, -1.7937e+00,\n",
       "          4.0170e-01, -1.5766e-01, -4.4044e-01, -8.1959e-01, -6.7362e-01,\n",
       "          2.4402e+00,  3.0291e-01, -4.6471e-01,  6.8012e-01, -7.6497e-01,\n",
       "         -1.4754e-01, -5.0006e-01,  7.8427e-01,  4.8301e-01,  1.4913e+00],\n",
       "        [ 1.3962e+00, -2.4624e+00, -5.9172e-01,  6.3084e-01, -4.2779e-01,\n",
       "          7.6133e-01, -4.5130e-01,  9.6284e-01,  1.9537e+00,  7.0915e-02,\n",
       "          1.1423e+00,  1.2488e+00, -5.5676e-01,  3.6725e-01,  1.6440e-01,\n",
       "          1.3245e-01,  7.2441e-01, -7.8642e-01,  9.4544e-02, -9.1741e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cfcc7-203d-46eb-b530-7ec4788ba6f6",
   "metadata": {},
   "source": [
    "## Using Broadcastable Matrix Multiplication\n",
    "### matmul is the most generic function that can perform everything above and more!\n",
    "* https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "* Performs a different operation depending on the input dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4ec5c-f332-4cc7-936a-6b5bb3c93325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLinearBroadcastable(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # YOU NEED TO RUN FOR INITIALIZATION OF MODULE\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_0 = nn.Parameter(torch.zeros(out_features))\n",
    "        # 2D parameter are required for matrix multiplication!\n",
    "        self.w_1 = nn.Parameter(torch.randn(out_features, in_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (N, S, F_in)\n",
    "        # w_1.shape: (F_out, F_in)\n",
    "        # x * w_1.T -> (N, S, F_in) * (F_in, F_out)\n",
    "        # .T mean transpose\n",
    "        # OPERATION IS BROADCASTED OVER \"S\" DIMENSION\n",
    "        y = torch.matmul(x, self.w_1.T) + self.w_0\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a10211cf-0ecb-4903-9f72-25d6bc2b2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_broadcastable = BasicLinearBroadcastable(in_features=20, out_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c05ef4fd-18f0-48c6-bcb8-c2f38579f357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_broadcastable(dummy_multidim_data_batch)\n",
    "pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
