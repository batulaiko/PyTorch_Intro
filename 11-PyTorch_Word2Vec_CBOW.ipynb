{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7984bef8-210c-4a5f-a2c1-99df31ccef0e",
   "metadata": {},
   "source": [
    "# Word2Vec CBOW Implementation using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae8b335-5eb6-451c-9d1d-3d674a349bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import strip_accents_ascii\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42f1f0-750f-4516-83a8-eb6f57f53c2e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17b4b1c-a934-4c32-9a00-93b9098c3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector size of each token\n",
    "EMBED_DIMENSION = 128\n",
    "# number of tokens from left and right\n",
    "CONTEXT_SIZE = 2\n",
    "BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c637085-7580-4687-af45-31ea094e3d1b",
   "metadata": {},
   "source": [
    "## Turkish sentences for word2vec trainingp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "523ecbdc-6e3a-48f5-a994-a842d11d9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from: https://www.kaggle.com/datasets/ahmetax/hury-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af892f93-db2f-4f26-97b8-e972837bc7a7",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d9cdec-4884-410a-9a3b-3b371763f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/turkish_sentences_for_word2vec_training/dunya-nz.txt', encoding='utf-8') as f:\n",
    "    wiki_train_raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1337a-0ed5-4a34-bc88-08312ef86caf",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a9dce9-5a24-4478-800e-f56a52f2bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d1d1ff-a396-4134-bb45-b4b115a62bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove accents\n",
    "    text = strip_accents_ascii(text)\n",
    "\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91806f3e-985d-44a2-90f0-f7bf78ef1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext_preprocessed_train = preprocess_text(wiki_train_raw)\n",
    "wikitext_tokens_train = word_tokenize(wikitext_preprocessed_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e75d1-aa3f-4b57-9164-e8ff8af3b822",
   "metadata": {},
   "source": [
    "### Build Vocabulary\n",
    "* **NOTE:** It is best to build vocab on the entire dataset (train/test combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d778edd-4a1f-40e4-a8d1-416c1bab40f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 226,473\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter(wikitext_tokens_train)\n",
    "\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common(n=None))}\n",
    "\n",
    "print(f'Vocab size: {len(vocab):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48412fa8-3fb3-4b8d-b940-3a504dfb73a7",
   "metadata": {},
   "source": [
    "### Convert word tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c49537-fdc7-4817-b718-123a904172b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens_train = [vocab[word] for word in wikitext_tokens_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ffdb5-ad83-409c-bdb6-b8a99fbbd7ae",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa16da3-40b9-4982-8997-85aeb8588fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, tokens, context_size):\n",
    "        self.tokens = tokens\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # (leave space for starting and ending context windows)\n",
    "        return len(self.tokens) - 2*self.context_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.tokens[idx:idx + self.context_size] + self.tokens[idx+self.context_size+1 : idx+2*self.context_size+1]\n",
    "        target = self.tokens[idx + self.context_size]\n",
    "        return torch.tensor(context), torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9757c7e5-3db1-421b-8c7d-d15109ca24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CBOWDataset(indexed_tokens_train, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199c4d7-493a-4afe-9edf-c3343ae17f71",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abef0ad8-47b5-4f1f-9dea-3f911026527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=8, # number of cpu cores\n",
    "    pin_memory=True, # faster GPU memory allocation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7031796-cb73-4693-ad4e-b7f753a21bc3",
   "metadata": {},
   "source": [
    "# Print Sample Context and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3a21603-bc2e-42d6-b8b3-bad1f4acf374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab IDs - Context: [3546  142  545  529]\n",
      "Vocab IDs - Target: 3\n",
      "String Tokens - Context: ['fransa', 'sonunda', 'olayn', 'umarm']\n",
      "String Token - Target: bu\n"
     ]
    }
   ],
   "source": [
    "for context, target in train_dataloader:\n",
    "    # Extract the first sample from the batch\n",
    "    # idx (integer)\n",
    "    context_sample = context[0].numpy()\n",
    "    target_sample = target[0].item()\n",
    "\n",
    "    # Convert vocab IDs to string tokens\n",
    "    context_tokens_str = [word for word, idx in vocab.items() if idx in context_sample]\n",
    "    target_token_str = [word for word, idx in vocab.items() if idx == target_sample][0]\n",
    "\n",
    "    print('Vocab IDs - Context:', context_sample)\n",
    "    print('Vocab IDs - Target:', target_sample)\n",
    "    print('String Tokens - Context:', context_tokens_str)\n",
    "    print('String Token - Target:', target_token_str)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28800bea-d089-4dde-a130-80de5548edc1",
   "metadata": {},
   "source": [
    "# Embedding Table for PyTorch\n",
    "* maps discrete token ids to continuous-valued vectors\n",
    "* It is like a matrix where rows are token ids, columns are vectors\n",
    "* It has \"weight\" as the learnble parameter (no bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e88bf90-1b28-44de-bccf-d90bb689b56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "# total 10 tokens\n",
    "_vocab_size = 10\n",
    "# dimension of vector for each token\n",
    "_emb_dim = 4\n",
    "\n",
    "emb = nn.Embedding(\n",
    "    num_embeddings=_vocab_size,\n",
    "    embedding_dim=_emb_dim,\n",
    ")\n",
    "\n",
    "print(emb.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5058df-4cbb-4cbd-9aa9-da1ce0892638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.9288, -1.0496,  1.0969,  0.3053],\n",
       "        [-0.3824,  0.9868,  0.1592, -2.7793],\n",
       "        [-0.0398, -0.0793,  0.6736,  0.0517],\n",
       "        [-0.1260, -0.9008, -0.0438,  0.4446],\n",
       "        [-1.2505,  0.3022,  0.6373,  1.2476],\n",
       "        [ 0.0953,  1.3099, -1.1293, -0.7064],\n",
       "        [-1.7267, -0.8775,  0.5312,  0.0625],\n",
       "        [-0.5806, -1.4279, -0.4218, -0.2073],\n",
       "        [ 1.3390,  0.6697,  0.6564, -1.1044],\n",
       "        [ 1.1431, -0.5799,  1.4895,  0.2775]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834ec11-1581-4b81-85d8-b9d4c434c6af",
   "metadata": {},
   "source": [
    "# CBOW Model\n",
    "*  https://arxiv.org/abs/1301.3781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce0ac76f-94f7-49ae-8ad1-34c34413838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_Model(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        x = self.embeddings(inputs_).mean(dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e116a1e-a7c9-4fd9-bd15-1ebaebf69882",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "674f8ae6-89a1-4000-ac72-275ce7299a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW_Model(vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320b284-bf8f-4f91-ad0b-3f4aa0f8a2d2",
   "metadata": {},
   "source": [
    "# Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa77fc0f-deb4-4a6a-9a31-c2fc5f129bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 12:42:11.537366: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-09 12:42:11.539088: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-09 12:42:11.562153: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-09 12:42:11.562176: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-09 12:42:11.562190: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-09 12:42:11.567429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-09 12:42:12.336469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad2ec4-7bc6-4eef-af76-336e518ab93b",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7c95f-f332-4101-a395-63df9474155d",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd35a8-f6d7-4d9b-b89d-3c150c980309",
   "metadata": {},
   "source": [
    "### Training iteration function that loops over all the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "641b411e-fa43-4547-93c8-90a9b9cebfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for context_tokens_batch, target_tokens_batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        \n",
    "        context_tokens_batch = context_tokens_batch.to(device)\n",
    "        target_tokens_batch = target_tokens_batch.to(device)\n",
    "            \n",
    "        # Forward pass\n",
    "        preds_tokens = model(context_tokens_batch)\n",
    "        # Compute error\n",
    "        loss = criterion(preds_tokens, target_tokens_batch)\n",
    "    \n",
    "        # Clear previously computed gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters (weights and biases)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(loss_history)/len(loss_history)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce374f46-4f26-4b44-8e93-d1d30c821043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(model, train_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loop over all epochs\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        avg_train_loss = train_iter(model, train_dataloader, criterion, optimizer, device)\n",
    "\n",
    "        print(f'Epoch: [{epoch+1}/{num_epochs}], Avg loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae393a8-3880-42f4-bf46-350c3b68610d",
   "metadata": {},
   "source": [
    "# Load Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d22ab06f-5dca-4f8d-966e-049a5b1645bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_filename = f'turkish_emb_{EMBED_DIMENSION}_vocab_{len(vocab)}'\n",
    "\n",
    "with open(f'{load_filename}.npy', 'rb') as f:\n",
    "    trained_embeddings = np.load(f)\n",
    "\n",
    "with open(f'vocab_emb_{EMBED_DIMENSION}_vocab_{len(vocab)}.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea6697-23c9-4bdc-bf2a-fd7532aa1a90",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "* This cell is commented\n",
    "* Change cell type to \"code\" to run this"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52d59717-3db2-4bfe-999d-5181a3359024",
   "metadata": {},
   "source": [
    "start_training(model, train_dataloader, optimizer, criterion, NUM_EPOCHS, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cad815-5234-46f7-98df-6bea6057f9f4",
   "metadata": {},
   "source": [
    "### Get Trained Embeddings\n",
    "* This cell is commented\n",
    "* Change cell type to \"code\" to run this"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f9f3b34-2c0e-46eb-9f2d-14788f1e7345",
   "metadata": {},
   "source": [
    "# Get the word vectors from nn.Embbeding table\n",
    "trained_embeddings = model.embeddings.weight.cpu().detach().clone().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4246e-c6b8-4f74-8d9f-a6dfd2b553a0",
   "metadata": {},
   "source": [
    "# Save Embeddings (Optional)\n",
    "* This cell is commented\n",
    "* Change cell type to \"code\" to run this"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9155f9b-634f-4300-8dda-33e713a55c29",
   "metadata": {},
   "source": [
    "save_filename = f'turkish_emb_{EMBED_DIMENSION}_vocab_{len(vocab)}'\n",
    "np.save(save_filename, trained_embeddings)\n",
    "\n",
    "with open(f'vocab_emb_{EMBED_DIMENSION}_vocab_{len(vocab)}.pkl', 'wb') as fp:\n",
    "    pickle.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a6eba-3d0c-4dbe-b522-a2ced3f22b13",
   "metadata": {},
   "source": [
    "# Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3329a24-2351-472b-b176-3ee9be7074bc",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "* Cosine similarity is a measure of similarity between two vectors in an inner product space. \n",
    "* In the context of natural language processing and text analysis, cosine similarity is commonly used to quantify the similarity between two documents or text passages\n",
    "* Cosine similarity is between -1.0 and 1.0.\n",
    "\n",
    "$ \\text{Cosine Similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\cdot \\|\\mathbf{B}\\|} $\n",
    "* $\\mathbf{A} \\cdot \\mathbf{B}$ represents dot product between vectors\n",
    "* $\\|\\mathbf{A}\\|$ and $\\|\\mathbf{B}\\|$ Euclidean norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f1dd6e5-b4ae-4da2-a82a-52d3f42cbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cosine_similarity(embedding1, embedding2):\n",
    "    dot_product = np.dot(embedding1, embedding2)\n",
    "    norm_embedding1 = np.linalg.norm(embedding1)\n",
    "    norm_embedding2 = np.linalg.norm(embedding2)\n",
    "    similarity = dot_product / (norm_embedding1 * norm_embedding2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbeae6-9e82-465a-a4a8-ee89114237ac",
   "metadata": {},
   "source": [
    "### Search similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdf7e149-8584-48ab-93a5-4d991e27d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity_search(query_word, embeddings, vocab, top_n=5):\n",
    "    # Check if the query word is in the vocabulary\n",
    "    if query_word not in vocab:\n",
    "        return f\"{query_word} is not in the vocabulary.\"\n",
    "\n",
    "    # Get the index of the query word in the vocabulary\n",
    "    query_idx = vocab[query_word]\n",
    "\n",
    "    # Extract the embedding of the query word\n",
    "    query_embedding = embeddings[query_idx]\n",
    "\n",
    "    # Calculate cosine similarity between the query embedding and all other embeddings\n",
    "    similarities = [custom_cosine_similarity(query_embedding, embedding) for embedding in embeddings]\n",
    "\n",
    "    # Get the indices of the top N similar words\n",
    "    # [::-1] -> reverse the order\n",
    "    # [:top_n] -> fetch top n\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "\n",
    "    # Extract the top N similar words and their cosine similarities\n",
    "    similar_words = [word for word, idx in vocab.items() if idx in top_indices and word != query_word]\n",
    "    cosine_similarities = np.array([similarities[idx] for idx in top_indices if idx != query_idx])\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Similar words to '{query_word}':\")\n",
    "    for word, similarity in zip(similar_words, cosine_similarities):\n",
    "        print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1344e-8486-4d7a-9570-8ac7ad2f63d1",
   "metadata": {},
   "source": [
    "### Example Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f6c61aa-27dd-4680-85e3-53a833b28cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'araba':\n",
      "arac: 0.5006\n",
      "otomobil: 0.4463\n",
      "silahn: 0.4312\n",
      "polimer: 0.3980\n"
     ]
    }
   ],
   "source": [
    "word_similarity_search(\"araba\", trained_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "719d1f54-9d84-4100-9853-ec5516205c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'gencler':\n",
      "ulkeler: 0.5031\n",
      "genclerin: 0.4317\n",
      "suriyeliler: 0.4253\n",
      "yps: 0.4098\n"
     ]
    }
   ],
   "source": [
    "word_similarity_search(\"gencler\", trained_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d374a9c-ca90-4e86-ab12-0e8e94716298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'televizyon':\n",
      "haber: 0.4532\n",
      "tv: 0.4438\n",
      "televizuyon: 0.4326\n",
      "chew: 0.4287\n"
     ]
    }
   ],
   "source": [
    "word_similarity_search(\"televizyon\", trained_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a528a158-d31f-4cea-9a2a-2d3e3ddcb9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'yol':\n",
      "yolunu: 0.5377\n",
      "onunu: 0.5261\n",
      "kaplarn: 0.4731\n",
      "kucak: 0.4593\n"
     ]
    }
   ],
   "source": [
    "word_similarity_search(\"yol\", trained_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6038c476-fc7d-4bfa-89a7-bbb9e36f28a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'ucak':\n",
      "ucagn: 0.5492\n",
      "gemi: 0.4722\n",
      "tren: 0.4618\n",
      "vinc: 0.4379\n"
     ]
    }
   ],
   "source": [
    "word_similarity_search(\"ucak\", trained_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e593304-458b-42b1-8c0e-3894f3562010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'meteoroloji':\n",
      "uzay: 0.4441\n",
      "itfaiye: 0.4354\n",
      "televizyonlarnda: 0.4044\n",
      "astrofizik: 0.4018\n"
     ]
    }
   ],
   "source": [
    "word_similarity_search(\"meteoroloji\", trained_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "863323d3-3a94-403b-8d74-a21e4f30469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'futbol':\n",
      "basketbol: 0.5539\n",
      "golf: 0.5095\n",
      "satranc: 0.4773\n",
      "voleybol: 0.4566\n"
     ]
    }
   ],
   "source": [
    "word_similarity_search(\"futbol\", trained_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda9520-1c74-4465-9401-5a8a4f9a262c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
