{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea71e1c7-ae41-4fa4-99b8-b89c4e2dfc77",
   "metadata": {},
   "source": [
    "# MNIST Image Classification with Convolutional Networks in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266c19bd-e085-4f0a-b2e6-b2b0be84e9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22d7d7-acdf-42d1-90fe-14f7c8cbe705",
   "metadata": {},
   "source": [
    "# MNIST Dataset \n",
    "* From PyTorch built-in datasets\n",
    "* convert images to tensors\n",
    "* normalize pixel values with a mean of 0.5 and a standard deviation of 0.5\n",
    "* Input is grayscale (single color channel, we have single values for mean and std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd85ec1-699e-4f0a-829a-fd7652d02411",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f74917-c899-45bd-9b77-74a3357fb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True,               \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bac5ef-20e1-4e4f-8418-cba656501ade",
   "metadata": {},
   "source": [
    "### See all classes\n",
    "* integer mapping of string names\n",
    "* (Remember models don't work with strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86854115-7e21-4256-a51f-2749b3238e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab94dd4f-610d-4524-8524-c21b7fa3fdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(train_dataset.classes)\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dfea0bf-5dc6-4d81-95a4-69347931cb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "ex_img, ex_target = train_dataset[0] # img, seg_mask\n",
    "\n",
    "print(ex_img.shape)\n",
    "print(ex_target) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2b5b0-1090-4ad1-bda0-138fc6285cc1",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac8df78-722b-4b54-9b68-6b92dc72ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09276768-cbf4-4506-ba71-378aa8ea646f",
   "metadata": {},
   "source": [
    "### Get sample batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a633bc5-b7d5-43a2-9ef6-6959b6540f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "ex_img_batch, ex_target_batch = next(iter(train_dataloader))\n",
    "print(ex_img_batch.shape)\n",
    "print(ex_target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1530837-98b8-429a-b2b0-356eb1edd436",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4d0c4-dc8d-4601-95a9-f3b988d888d8",
   "metadata": {},
   "source": [
    "### Convolution Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9987370-f5f7-4f54-b623-914618dcf456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_r = torch.randn(8, 1, 28, 28)\n",
    "x_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0011d404-342b-4dac-8eb6-5aca27fff869",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv_layer = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e58358ab-dad7-4664-9c5f-8edc0ac4a622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 26, 26])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = test_conv_layer(x_r)\n",
    "test_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee3497-936a-4fde-b170-5beda115d904",
   "metadata": {},
   "source": [
    "### Max Pooling Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2b703d-97b1-42a3-b718-438fc08939ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pooling_layer = nn.MaxPool2d(kernel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34689b43-2ee2-43c8-89db-6b97a1d4b214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 14, 14])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output2 = test_pooling_layer(x_r)\n",
    "test_output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a840a5-0798-4801-bce0-b082ae5e4ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 13, 13])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output3 = test_pooling_layer(test_output)\n",
    "test_output3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2c7d8-2687-47d0-882d-b3b2d2126ec3",
   "metadata": {},
   "source": [
    "### Convolution Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d32c0fc-5d30-4ae3-8952-2139d044a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #######################\n",
    "        # Convolutional Part\n",
    "        #######################\n",
    "        #print(f'Input dims: {x.shape}')\n",
    "        \n",
    "        x = self.conv1(x) # (N, 1, 28, 28) -> (N, 32, 26, 26)\n",
    "        #print(f'After conv1 {x.shape}')\n",
    "        x = self.relu(x) # no dim change\n",
    "        x = self.conv2(x) # (N, 32, 26, 26) -> (N, 64, 24, 24)\n",
    "        #print(f'After conv2 {x.shape}')\n",
    "        x = self.relu(x) # no dim change\n",
    "        x = self.max_pool(x) # (N, 64, 24, 24) -> (N, 64, 12, 12)\n",
    "        #print(f'After maxpool {x.shape}')\n",
    "        #######################\n",
    "        #######################\n",
    "\n",
    "        #######################\n",
    "        ## Fully Connected Part\n",
    "        #######################\n",
    "        x = torch.flatten(x, 1) # (N, 64, 12, 12) -> (N, 64*12*12) -> (N, 9216)\n",
    "        x = self.fc1(x) # (N, 9216) -> (N, 128)\n",
    "        x = self.relu(x) # no dim change\n",
    "        logits = self.fc2(x) # (N, 128) - (N, 10)\n",
    "        #######################\n",
    "        #######################\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8bff8-0909-4a56-a0ef-a93383dc5f85",
   "metadata": {},
   "source": [
    "### Dummy Input for Dimentional Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa53ac93-5ff9-407a-b317-0b29bf9b03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(\n",
    "    input_channels=1, # 1 for grayscale images \n",
    "    num_classes=NUM_CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa199a8-3bad-44d1-b20f-1394d4c13e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73fdfd1a-5b83-4a89-a385-ae50cf3d2f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_preds = model(dummy_input)\n",
    "dummy_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af089da4-9faf-4345-ab35-cb06f15ed7ea",
   "metadata": {},
   "source": [
    "## Print Model Parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69eec7c4-8738-4e74-a8dc-873dc15c39df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 9216])\n",
      "torch.Size([128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a18b9e-6ef5-4d35-9f32-2d68d6eebda4",
   "metadata": {},
   "source": [
    "## Print with Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aa09b96-ab09-437f-9694-e193a1d553b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: conv1.weight and parameter data: torch.Size([32, 1, 3, 3])\n",
      "\n",
      "name: conv1.bias and parameter data: torch.Size([32])\n",
      "\n",
      "name: conv2.weight and parameter data: torch.Size([64, 32, 3, 3])\n",
      "\n",
      "name: conv2.bias and parameter data: torch.Size([64])\n",
      "\n",
      "name: fc1.weight and parameter data: torch.Size([128, 9216])\n",
      "\n",
      "name: fc1.bias and parameter data: torch.Size([128])\n",
      "\n",
      "name: fc2.weight and parameter data: torch.Size([10, 128])\n",
      "\n",
      "name: fc2.bias and parameter data: torch.Size([10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(f'name: {n} and parameter data: {p.shape}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbcdf7-8fb8-4806-802d-517e93cc5bb8",
   "metadata": {},
   "source": [
    "# Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbb57fef-8e78-4933-9bba-d233099fef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "learning_rate = 0.02\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Negative log-likehood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074574d-6d9f-4e24-b31c-7b632b4a0d23",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c63a8d8f-ae53-4779-8fdf-a73c510cf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for batch_idx, (img, target) in enumerate(train_loader):\n",
    "        # Move to GPU (if available)\n",
    "        img = img.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(img)\n",
    "        # Compute gradients\n",
    "        loss = criterion(preds, target)\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        # In PyTorch, gradients are accumulated, you need to reset gradients in each loop\n",
    "        optimizer.zero_grad()\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters (weights and biases)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(loss_history)/len(loss_history)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d7a8f3-97a9-47c4-ae8f-152cff9f43fa",
   "metadata": {},
   "source": [
    "# Testing\n",
    "* No trainin in testing code\n",
    "* Disable Autograd\n",
    "* No optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9681684-6f03-4315-bc03-137f0f6c4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    \n",
    "    for img, target in test_loader:\n",
    "        # Move to GPU (if available)\n",
    "        img = img.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(img)\n",
    "        # Compute error\n",
    "        loss = criterion(preds, target)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(preds, 1)\n",
    "        accuracy = (predicted == target).sum().item() / target.size(0)\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        acc_history.append(accuracy)\n",
    "    \n",
    "    avg_loss = sum(loss_history)/len(loss_history)\n",
    "    avg_acc = sum(acc_history)/len(acc_history)\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4709d-ff10-42ff-9bf2-d8452c3ff7fb",
   "metadata": {},
   "source": [
    "### Start Training\r\n",
    "* Training consists of two steps: forward and backward propagation\r\n",
    "* In forward propagation, we input the data into the model and measure the error (with loss function)\r\n",
    "* In backward propagation, we adjust the internal paramters of the model so that model makes better predictions next time\r\n",
    "* One complete cycle of the dataset is called \"epoch\" (one loop cycle of all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de20bdf1-d69b-491d-a16c-d5fa8d33e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(model, train_dataloader, test_dataloader, optimizer, criterion, num_epochs, print_interval):\n",
    "\n",
    "    # Loop over all epochs\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        avg_train_loss = train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "        avg_test_loss, avg_test_acc = test(model, test_dataloader, criterion)\n",
    "\n",
    "        if (epoch + 1) % print_interval == 0:\n",
    "            print(f'Epoch: [{epoch+1}/{num_epochs}], Avg train loss: {avg_train_loss:.4f}, test loss: {avg_test_loss:.4f}, test_acc: {avg_test_acc*100.0:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eedfd3d1-ebbd-4182-9369-862f9e265935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/10], Avg train loss: 0.2647, test loss: 0.0880, test_acc: 97.26%\n",
      "Epoch: [4/10], Avg train loss: 0.0483, test loss: 0.0454, test_acc: 98.52%\n",
      "Epoch: [6/10], Avg train loss: 0.0288, test loss: 0.0379, test_acc: 98.76%\n",
      "Epoch: [8/10], Avg train loss: 0.0181, test loss: 0.0384, test_acc: 98.70%\n",
      "Epoch: [10/10], Avg train loss: 0.0112, test loss: 0.0417, test_acc: 98.74%\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "print_interval = 2 \n",
    "\n",
    "start_training(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    NUM_EPOCHS,\n",
    "    print_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9dd1d-1a41-4566-b221-797a810db108",
   "metadata": {},
   "source": [
    "# Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd05fecf-fb3c-45b9-beba-e738d8712d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "},\n",
    "    'convnet_mnist_checkpoint.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b125f-6a85-44dd-b322-64e01b61fa6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
